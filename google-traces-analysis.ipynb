{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Análise de traços de execução</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Installs de bibliotecas necessárias</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: pyspark_dist_explore in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: pyspark in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark_dist_explore) (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark_dist_explore) (3.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark_dist_explore) (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark_dist_explore) (1.22.4)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pyspark_dist_explore) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pyspark_dist_explore) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pyspark_dist_explore) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pyspark_dist_explore) (4.33.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pyspark_dist_explore) (9.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pyspark_dist_explore) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pyspark_dist_explore) (1.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->pyspark_dist_explore) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\322010\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->pyspark_dist_explore) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\322010\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark pyspark_dist_explore pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Primeiro utilizamos o findspark para que o ambiente saiba onde o spark está localizado.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports(têm de ser feitos após o init do findspark)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark_dist_explore import hist\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "import math\n",
    "import sys,os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType,StructField, LongType, IntegerType,FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Inicializamos uma seção Spark, ou pegamos a que está atualmente em execução</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"Sessao\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Definimos um schema para o RDD</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_events_schema = StructType([ \\\n",
    "    StructField(\"time\",LongType(),True), \\\n",
    "    StructField(\"type\",IntegerType(),True), \\\n",
    "    StructField(\"collection_id\",IntegerType(),False), \\\n",
    "    StructField(\"priority\", IntegerType(), True), \\\n",
    "    StructField(\"instance_index\", IntegerType(), False), \\\n",
    "    StructField(\"cpu_resource_request\", FloatType(), True), \\\n",
    "    StructField(\"memory_resource_request\", FloatType(), True) \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Carregamos um arquivo CSV em um RDD(sem cabeçalho e com o schema definido)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCE = spark.read.option(\"header\",\"true\").schema(instance_events_schema).csv(\"instance_events/instance_events-000000000000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Média dos requerimentos de utilização de memória(por tipo de \"coisa\")</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ao persistir o RDD, operações subsequentes reutilizarão os dados relativos ao RDD em operações que o envolvam, diminuindo drasticamente o tempo de execução das mesmas</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time: bigint, type: int, collection_id: int, priority: int, instance_index: int, cpu_resource_request: float, memory_resource_request: float]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCE.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Essa é executada sem cache</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------+\n",
      "|type|avg(memory_resource_request)|\n",
      "+----+----------------------------+\n",
      "|   0|        0.003215060709829252|\n",
      "|   1|         0.01933770519928046|\n",
      "|   2|        0.003345685608592...|\n",
      "|   3|        0.003335462585395...|\n",
      "|   4|        0.003473938670384...|\n",
      "|   5|        0.020445465992991966|\n",
      "|   6|        0.004764337917852076|\n",
      "|   7|        0.002935408097730315|\n",
      "|   8|        0.005776905272776365|\n",
      "|   9|        0.003988534368734...|\n",
      "|  10|        0.008756889907134162|\n",
      "+----+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddCE.orderBy('type').groupBy('type').agg({\"`memory_resource_request`\":'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Essa é executada COM cache</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------+\n",
      "|type|avg(cpu_resource_request)|\n",
      "+----+-------------------------+\n",
      "|   0|     0.009525861089444346|\n",
      "|   1|     0.009540171341634771|\n",
      "|   2|     0.009649705901209166|\n",
      "|   3|     0.009805624388920475|\n",
      "|   4|     0.009645259130290452|\n",
      "|   5|     0.008741734786721873|\n",
      "|   6|     0.014509926217036775|\n",
      "|   7|     0.009677616500338168|\n",
      "|   8|     0.018229468438387228|\n",
      "|   9|     0.013241837398293013|\n",
      "|  10|     0.011053578572176771|\n",
      "+----+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddCE.orderBy('type').groupBy('type').agg({\"`cpu_resource_request`\":'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>filtra os eventos de tasks com tipo=3(submissão), com valores de tempo dentro do intervalo observado no traço (0<t<MAXINT>>)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCE = rddCE.filter((rddCE.time.isNotNull()) & (rddCE.type==3) & (rddCE.time>0) & (rddCE.time<sys.maxsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converte microssegundo em hora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def microToHour(x):\n",
    "    return math.floor(x/3.6e+9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define o objeto udf, que pode ser utilizado pra aplicar a função microToHour no RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_hours = udf(lambda x:microToHour(x),IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cria-se nova coluna com o tempo em horas e adiciona-a ao RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddWithH = rddCE.withColumn(\"hour\",udf_hours(col(\"time\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acha as máximas e mínimas das horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604799625804, 600777673)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxTime = rddCE.agg({'time':'max'}).collect()[0][0]\n",
    "minTime = rddCE.agg({'time':'min'}).collect()[0][0]\n",
    "maxTime,minTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11904.419161676647"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCE.agg({'time':'count'}).collect()[0][0]/(microToHour(maxTime)-microToHour(minTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontra a quantidade de tasks submetidas por hora, ao iterar por todos os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600001531, 604799934699, 429141.46706586826)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTime = sys.maxsize\n",
    "maxTime = 0\n",
    "type3Count = 0\n",
    "for filename in os.listdir(\"instance_events\"):\n",
    "    f = os.path.join(\"instance_events\",filename)\n",
    "    rddCE = spark.read.option(\"header\",\"true\").schema(instance_events_schema).csv(f)\n",
    "    rddCE.persist()\n",
    "    rddCE = rddCE.filter((rddCE.time.isNotNull()) & (rddCE.type==3) & (rddCE.time>0) & (rddCE.time<sys.maxsize))\n",
    "    type3Count = type3Count+rddCE.agg({'time':'count'}).collect()[0][0]\n",
    "    if rddCE.agg({'time':'count'}).collect()[0][0]>0:\n",
    "        maxTime = max(maxTime,rddCE.agg({'time':'max'}).collect()[0][0])\n",
    "        minTime = min(minTime,rddCE.agg({'time':'min'}).collect()[0][0])\n",
    "    \n",
    "minTime,maxTime,type3Count/(microToHour(maxTime)-microToHour(minTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCE.agg({'time':'count'}).collect()[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa8ee55327338118c9b4517d613770d195c366ac837f284f64f7c768461f394a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
